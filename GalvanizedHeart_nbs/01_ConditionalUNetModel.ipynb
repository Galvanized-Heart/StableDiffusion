{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU is there'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"GPU is there\" if torch.cuda.is_available() else Exception(\"GPU is missing\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Conditional UNet from Lecture 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(tsteps, emb_dim, max_period=10000):\n",
    "    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n",
    "    emb = tsteps[:,None].float() * exponent.exp()[None,:]\n",
    "    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "    return F.pad(emb, (0,1,0,0)) if emb_dim%2==1 else emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(ni, nf, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ni, nf, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads_to_batch(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(n, sl, heads, -1)\n",
    "    return x.transpose(2, 1).reshape(n*heads,sl,-1)\n",
    "\n",
    "def batch_to_heads(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1,sl,d*heads)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ni//attn_chans\n",
    "        self.scale = math.sqrt(ni/self.nheads)\n",
    "        self.norm = nn.LayerNorm(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,s = x.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.qkv(x)\n",
    "        if self.nheads != 1: x = heads_to_batch(x, self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        if self.nheads != 1: x = batch_to_heads(x, self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class SelfAttention2D(SelfAttention):\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        return super().forward(x.view(n, c, -1)).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_chans=0):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        self.emb_proj = nn.Linear(n_emb, nf*2)\n",
    "        self.conv1 = pre_conv(ni, nf, ks, act=act, norm=norm)\n",
    "        self.conv2 = pre_conv(nf, nf, ks, act=act, norm=norm)\n",
    "        self.idconv = noop() if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "        self.attn = False\n",
    "        if attn_chans: self.attn = SelfAttention2D(nf, attn_chans)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        emb = self.emb_proj(F.silu(emb))[:, :, None, None]\n",
    "        scale, shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.idconv(inp)\n",
    "        if self.attn: x = x + self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saved(m, blk):\n",
    "    m_ = m.forward\n",
    "\n",
    "    @wraps(m.forward)\n",
    "    def _f(*args, **kwargs):\n",
    "        res = m_(*args, **kwargs)\n",
    "        blk.saved.append(res)\n",
    "        return res\n",
    "\n",
    "    m.forward = _f\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf, attn_chans=attn_chans), self)\n",
    "                                      for i in range(num_layers)])\n",
    "        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        self.saved = []\n",
    "        for resnet in self.resnets: x = resnet(x, emb)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf, attn_chans=attn_chans)\n",
    "            for i in range(num_layers)])\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t, ups):\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondUNetModel(nn.Module):\n",
    "    def __init__( self, n_classes, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.cond_emb = nn.Embedding(n_classes, n_emb)\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x,t,c = inp\n",
    "        temb = timestep_embedding(t, self.n_temb)\n",
    "        cemb = self.cond_emb(c)\n",
    "        emb = self.emb_mlp(temb) + cemb\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Embs from Lecture 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters for each layer of UNet\n",
    "nfs = (224,448,672,896)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding sizes\n",
    "n_temb = nf = nfs[0]\n",
    "n_emb = nf*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up testing data\n",
    "x0 = torch.randn(64,3,28,28) # mock batch of images\n",
    "n = len(x0) # batch of 64\n",
    "t = torch.rand(n,).to(x0).clamp(0,0.999) # 64 random timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]), torch.Size([64, 3, 28, 28]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 224])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get timestep embs\n",
    "t_emb = timestep_embedding(t, nf)\n",
    "t_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 896])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass through linear layers to get same size as conditional embs\n",
    "emb_mlp = nn.Sequential(lin(n_temb, n_emb, norm=nn.BatchNorm1d), lin(n_emb, n_emb))\n",
    "t_emb_mlp = emb_mlp(t_emb)\n",
    "t_emb_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([896])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get conditional embs\n",
    "n_classes = 10\n",
    "cond_emb = nn.Embedding(n_classes, n_emb)\n",
    "class_id = torch.tensor(0)\n",
    "c_emb = cond_emb(class_id)\n",
    "c_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 896])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get combined embs\n",
    "emb = t_emb_mlp + c_emb\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 448, 1, 1])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Silu + Linear layer\n",
    "linear = nn.Linear(n_emb, nf*2) # Double it here to get enough embeddings for scale and shift!\n",
    "emb2 = linear(F.silu(emb))[:, :, None, None]\n",
    "emb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 224, 1, 1]), torch.Size([64, 224, 1, 1]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get transformations from embs\n",
    "scale, shift = torch.chunk(emb2, 2, dim=1)\n",
    "scale.shape, shift.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 224, 28, 28])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforms are meant to end up having same num_filters that x will take on \n",
    "# after being passed through conv1 in ResBlock (e.g. 224 in this case)\n",
    "ni = x0.shape[1]\n",
    "ks = 3\n",
    "conv1 = pre_conv(ni, nf, ks, act=nn.SiLU, norm=nn.BatchNorm2d)\n",
    "x = conv1(x0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 224, 28, 28])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply transformations\n",
    "x = x*(1+scale) + shift # From what I understand, this can be subject to creativity\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 224, 28, 28])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finish ResBlock\n",
    "conv2 = pre_conv(nf, nf, ks, act=nn.SiLU, norm=nn.BatchNorm2d)\n",
    "x = conv2(x)\n",
    "convid = nn.Conv2d(ni, nf, 1)\n",
    "x = conv2(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
