{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d84e9d-7035-456a-ba90-53e6eb50306d",
   "metadata": {},
   "source": [
    "## This nb borrowed lines from fastai, OpenAI & ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff34ba2e-329b-417e-a8e4-c32077a93aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d8eba6-0cb1-4b2b-a28c-e80424012298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc\n",
    "from miniai.datasets import *\n",
    "from miniai.training import *\n",
    "\n",
    "from datasets import load_dataset,load_dataset_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b224b5-946e-4a51-91de-6ae5b786349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c572fa-7e44-427a-a636-7275a85fe74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of GPUs available =  1\n"
     ]
    }
   ],
   "source": [
    "class SomeException(BaseException):\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        print('# of GPUs available = ', torch.cuda.device_count())\n",
    "    else:\n",
    "        raise SomeException\n",
    "except SomeException:\n",
    "    print(\"ERROR: GPU is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdc6c4-776d-4259-ab47-5a7009bec5dc",
   "metadata": {},
   "source": [
    "### Loading Tiny-Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2d6701-0e68-431d-acbb-02830249de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"zh-plus/tiny-imagenet\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33bdd71-1e5c-495b-850d-e95ef6343b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca58619-1c05-475b-b9e5-a2eec5308c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyHFDS:\n",
    "    # this is specific for Hugging Face's dsd which can be very slow to index\n",
    "    def __init__(self, hg_ds):\n",
    "        self.ds = hg_ds\n",
    "        # the 2 lines below must be done here because the execution of \"ds['train']['image']\" takes 18seconds\n",
    "        # once it has executed, indexing it is very fast (5 milliseconds)\n",
    "        self.image = hg_ds['image']\n",
    "        self.label = hg_ds['label']\n",
    "    def __len__(self): return self.ds.num_rows\n",
    "    def __getitem__(self, i): return TF.to_tensor(self.image[i]), self.label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b117d-39e0-43ac-bd50-1f982e9348c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = TinyHFDS(dsd['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32827598-0dc9-48e6-94f3-281bbda86ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = TinyHFDS(dsd['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52472aa5-8625-4984-88e8-c060728801ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfmDS:\n",
    "    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        x,y = self.ds[i]\n",
    "        return self.tfmx(x),self.tfmy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68b5de-8e8a-445a-a185-56b291fd4255",
   "metadata": {},
   "source": [
    "### TODO: Add normalisation to datasets, see Jeremy's nb_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f423356-8075-4fbb-96d8-b95fed286801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfmx(x):\n",
    "    try:\n",
    "        if len(x.shape) == 3 and x.shape[0] == 3:\n",
    "            pass\n",
    "        else:\n",
    "            x = torch.ones([3,1,1]) * x\n",
    "    except SomeException:\n",
    "        print(\"ERROR: x doesn't have 3 channels\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbabf51-0bd4-4060-9c7c-70aa173343a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfmy(y): \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc0582-d2ef-4996-abda-b9cc30a6989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_tds = TfmDS(tds, tfmx, tfmy)\n",
    "tfm_vds = TfmDS(vds, tfmx, tfmy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2f189-1bb0-44fd-a8ee-1498e845b875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eeddce-48b5-491d-adc7-31da319fe9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e835f-7099-447b-9419-9efbbe1370ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xb[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13780b5b-9fdc-4e32-9042-340b7808e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306eddf-a42d-4179-9b9b-54e8579ca259",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xb[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300aae2-b5e8-46d7-b704-931c4c6513b7",
   "metadata": {},
   "source": [
    "### BROKEN: Generate captions for CLIP to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f566bd-e6df-4e55-8c0f-e257c8b21e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files\n",
    "fp_json_pos2idx = hf_hub_download(repo_id=name, filename=\"dataset_infos.json\", repo_type=\"dataset\")\n",
    "fp_idx2human = hf_hub_download(repo_id=name, filename=\"classes.py\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155fa3d-af77-445c-8c9e-5b27a3161a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(fp_json_pos2idx,) \n",
    "data_pos2idx = json.load(f) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd41d7f-d5e8-4c8a-b60c-9a788ae539ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo provided a variable called 'i2d'\n",
    "exec(open(fp_idx2human).read()) \n",
    "# Let's rename this variable to something suitable for this notebook\n",
    "idx2human = i2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd24cee-ec67-4852-9444-cb8d1f70e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2idx = data_pos2idx['Maysee--tiny-imagenet']['features']['label']['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630b075-ee0d-4d9d-8ba2-ddf182970cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2human = [idx2human[v] for k,v in enumerate(pos2idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84accf4e-d78b-4cd1-80b8-0e07f76b09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2human[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94efd5-841c-479b-8dae-bebbd7e8e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\"A photo of a \" + txt.split(\",\")[0] for txt in pos2human]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a9e2e-4492-4455-98e0-d1ac3e971175",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30c808-c872-443e-a952-1ecc284d17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfbcfc-ba4b-47cf-8097-c93079c021ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c25a8-e0ff-4d8f-9911-8becd5f8c3ff",
   "metadata": {},
   "source": [
    "### Randomly choose an image to pass to CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926d4b7-1c40-4dde-8d81-2ef90f747b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=(torch.rand(1)*15).round().int().item()\n",
    "selected_image = xb[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48925c38-d2c1-440c-a7a9-99b7bafe56d2",
   "metadata": {},
   "source": [
    "\"do_rescale=False\" is required for the next cell.\n",
    "CLIP's predictions are worse by a big margin if input image is rescaled a second time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502d313-f797-4222-8aed-c598e78d3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=captions, images=selected_image, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "image_input = inputs[\"pixel_values\"].to(\"cuda\")\n",
    "text_inputs = inputs[\"input_ids\"].to(\"cuda\")\n",
    "attention_mask = inputs[\"attention_mask\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e704263-8c3c-4717-8c6e-27f280dc64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(selected_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed280f57-9ac3-4dad-8511-128d4b764f0e",
   "metadata": {},
   "source": [
    "### CLIP: (img, text) -> (img_emb, text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59495162-7920-4158-875a-45968f2003c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Compute the image and text embeddings\n",
    "    image_features = model.get_image_features(image_input)\n",
    "    text_features = model.get_text_features(input_ids=text_inputs, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cf044-cb46-4a7e-b9f2-b8d2f18fbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86efee1-3137-491e-bae6-4e3f7e56ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd704cac-660d-49cd-9d50-e8f6f89197fd",
   "metadata": {},
   "source": [
    "### CLIP: dot product and find best 5 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076bb60-1998-4fc1-a97e-08b874797913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity scores\n",
    "similarity_scores = image_features @ text_features.T\n",
    "\n",
    "# Get the index of the best matching text description\n",
    "best_match_index = similarity_scores.argmax().item()\n",
    "\n",
    "# Output the best matching text description\n",
    "print(f\"Best match: {captions[best_match_index]} (score: {similarity_scores[0, best_match_index]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be9219-9505-4f0e-a1b5-0e4d2aa2a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, indx = similarity_scores[0].topk(5)\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(vals, indx):\n",
    "    print(f\"{captions[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8baf1-c556-487d-b348-d0a6369c7b72",
   "metadata": {},
   "source": [
    "### True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c581f-1000-4641-8de7-35c2dbf20b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions[yb[pos]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53153150-4670-4797-ae8b-8efdf7cfc723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8eeaeb-fdfa-4647-a381-d1423480dbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast.ai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
